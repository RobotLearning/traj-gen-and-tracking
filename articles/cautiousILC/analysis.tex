\section{Analysis}\label{analysis}
%
In this section we analyze and prove the monotonic convergence of the pseudoinverse-based ILC in \eqref{pseudoinverseILC2} and TLS-based ILC in \eqref{tlsilc2} under certain conditions. To the best of our knowledge, these bounds for satisfying monotonic convergence are new. These bounds will be seen to be more restrictive for pseudoinverse-based ILC. Furthermore, we indicate the effects of truncation to convergence and stability. For the readers convenience, we will be as self-contained as possible. We consider the usual Euclidean norm, i.e the 2-norm, as our vector norm $\|\cdot\|$. We start by analyzing the conditions for stability in the iteration domain. 

% first order vs. higher order?
% Q matrix?
\begin{defn} An ILC update law of the form $\sysInput_{k+1} = \sysInput_{k} - \lmatrix \error_{k}$ with $\lmatrix \in \mathbb{R}^{n \times m}$ operating on the unknown system with response $\error_k = \latentMat \sysInput_k + \linDist$ is said to be \emph{asymptotically stable} in the iteration-domain if the control inputs are bounded: $\exists M \in \mathbb{R} \ $ s.t. $\ \|\sysInput_k\| < M, \ \forall k \in \mathbb{Z}^{+}$ and the limit exists: $\lim\limits_{k \to \infty}\sysInput_k = \sysInput_{\infty} \in \mathbb{R}^{n}$. \end{defn}
%
We refer to the input-output matrix $\latentMat$ of the unknown system as the \emph{latent matrix} and $\lmatrix$ as the \emph{learning matrix}. Conditions for ensuring asymptotic stability are given below.
%
\begin{lem}[AS] \label{AS} A system operating under an ILC update law is asymptotically stable (AS) if the spectral radius $\rho = |\lambda_{\mathrm{max}}(\vec{I} - \lmatrix\latentMat)| < 1$. \end{lem}
%
\begin{proof}
Taking the dynamics $\error_k = \latentMat \sysInput_k + \linDist$ for an arbitrary initial guess $\sysInput_0 \in \mathbb{R}^{n}$
\begin{align}
\sysInput_1 &= \sysInput_0 - \lmatrix\error_0 \\
            &= (\vec{I} - \lmatrix\latentMat)\sysInput_0 - \lmatrix\linDist, \\
\sysInput_2 &= \sysInput_1 - \lmatrix(\latentMat\sysInput_1 + \linDist) \\ 
&= (\vec{I} - \lmatrix\latentMat)^{2}\sysInput_0 - (\vec{I} - \lmatrix\latentMat)\lmatrix\linDist - \lmatrix\linDist.
\end{align}
%
\noindent Hence for an arbitrary $k \in \mathbb{Z}^{+}$
%
\begin{align}
\sysInput_{k+1} &= (\vec{I} - \lmatrix\latentMat)\sysInput_{k} - \lmatrix\linDist, \\
\sysInput_k &= (\vec{I} - \lmatrix\latentMat)^{k}\sysInput_0 - \left(\sum_{l=0}^{k-1}(\vec{I} - \lmatrix\latentMat)^{l}\right)\lmatrix\linDist.
\end{align}
%
\noindent The series $\{\sysInput_{k}\}_{k=1}^{\infty}$ is bounded and convergent if and only if the spectral radius $\rho < 1$. Moreover the matrix $\lmatrix\latentMat$ is invertible since we can find a matrix norm $\|\cdot\|$ with $\rho \leq \|\vec{I} - \lmatrix\latentMat\| \leq \rho + \delta$ for some $\delta < 1 - \rho$ which implies $(\lmatrix\latentMat)^{-1} = \sum_{l=0}^{\infty}(\vec{I} - \lmatrix\latentMat)^{l}$. The limit is the fixed point of the series
% put reference for invertibility
%
\begin{align}
\sysInput_{\infty} &= (\vec{I} - \lmatrix\latentMat)\sysInput_{\infty} - \lmatrix\linDist, \\
\sysInput_{\infty} &= -(\lmatrix\latentMat)^{-1}\lmatrix\linDist.
\end{align}
%
\end{proof}
%
Note that $\latentMat$ needs to be of full rank for the spectral radius to be less than one. We can now easily show the steady-state error of the ILC law.
%
\begin{cor}
For a system that is asymptotically stable in the iteration-domain, the steady-state error is finite: $\error_{\infty} = (\vec{I} - \latentMat(\lmatrix\latentMat)^{-1}\lmatrix)\linDist$. In particular, $\projOblique = (\vec{I} - \latentMat(\lmatrix\latentMat)^{-1}\lmatrix)$ is an oblique projection matrix onto $\mathrm{null}(\latentMat^{\mathrm{T}})$ and is orthogonal if $\lmatrix = \latentMat^{\dagger}$ or in general if $\lmatrix = (\systemMat^{\mathrm{T}}\systemMat + \vec{R})^{-1}\systemMat^{\mathrm{T}}$ for $\latentMat = \systemMat\vec{M}$, $\vec{M} \in \mathbb{R}^{n \times n}$ nonsingular and $\systemMat$ full rank.
\end{cor}
%
\begin{proof}
%
\begin{align}
\error_{\infty} &= \latentMat\sysInput_{\infty} + \linDist, \\
\error_{\infty} &= -\latentMat(\lmatrix\latentMat)^{-1}\lmatrix\linDist + \linDist = (\vec{I} - \latentMat(\lmatrix\latentMat)^{-1}\lmatrix)\linDist.
\end{align}
%
Conditions for the projection matrix can be easily checked. For dynamics $\latentMat = \systemMat\vec{M}$ of full rank and suitable $\vec{R}$
%
\begin{align}
\error_{\infty} &= (\vec{I} - \latentMat\vec{M}^{-1}(\systemMat^{\mathrm{T}}\systemMat)^{-1}(\systemMat^{\mathrm{T}}\systemMat + \vec{R})^{-1}\lmatrix\linDist, \\
&= (\vec{I} - \latentMat\vec{M}^{-1}\vec{M}(\latentMat^{\mathrm{T}}\latentMat)^{-1}\vec{M}^{\mathrm{T}}\vec{M}^{-\mathrm{T}}\latentMat^{\mathrm{T}}), \\
&= (\vec{I} - \latentMat\latentMat^{\dagger})\linDist.
\end{align}
%
\end{proof}
%
The corollary states that to get a steady-state error of smallest norm, we don't need to know the exact dynamics of the system but that it should be in the range of the nominal dynamics. However, conditions for asymptotic stability need to be satisfied. Regularization does not affect the steady-state error.

With Lemma~\ref{AS} it is difficult to come up with a constructive criterion for AS. Especially we would like to obtain a more easily checked AS criterion for the pseudoinverse of $\systemMat$ when the underlying system $\latentMat$ is close to $\systemMat$, e.g. $\latentMat = \systemMat + \errorMat$ for some error $\errorMat$ with bounded spectral norm, $\|\errorMat\|_2 < M$, with $M \in \mathbb{R}$ small. We now give such a (sufficient) criterion for AS.
%
\begin{lem}\label{AS-pinv}
A system operating under the pseudoinverse update law \eqref{pseudoinverseILC2} is asymptotically stable (AS) if the matrices $\systemMat$, $\latentMat$ are full rank, i.e. $\mathrm{rank}(\systemMat) = \mathrm{rank}(\latentMat) = n$ and $\|\errorMat\|_2 = \sigma_{\mathrm{max}}(\errorMat) < \sigma_{\mathrm{min}}(\systemMat) = 1/\|\systemMat^{\dagger}\|_2$.
\end{lem}
%
\begin{proof}
It is sufficient to bound the spectral radius with the spectral norm and show that it is less than one. The fact that $\systemMat$ is of full column rank simplifies things quite significantly. Using $\systemMat^{\dagger}\systemMat = \vec{I}$
%
\begin{align}
\rho(\vec{I} - \lmatrix\latentMat) &\leq \|\vec{I} - \lmatrix\latentMat\|_2 = \|\vec{I} - \systemMat^{\dagger}(\systemMat + \errorMat)\|_2 \\
&= \|\systemMat^{\dagger}\errorMat\|_2 \leq \|\systemMat^{\dagger}\|_2\|\errorMat\|_2,
\end{align}
%
which is less than one if $\|\errorMat\|_2 < 1/\|\systemMat^{\dagger}\|_2$.
\end{proof}
%
Intuitively, Lemma~\ref{AS-pinv} states that the sign of the derivative should be known~\cite{Kolter09}. However this is not enough to guarantee \emph{monotonicity}. Generally an asymptotically stable ILC algorithm may not be monotonic.
%
\begin{defn}
An AS system operating under an ILC update law is monotonically convergent (MC) with rate $1/\gamma$, $0 < \gamma < 1$, if $\|\error_{k+1} - \error_{\infty}\| < \gamma \|\error_{k} - \error_{\infty}\|$ for all $k \in \mathbb{Z}^{+}$.
\end{defn}
%
The bounds on the error matrix $\errorMat$ guaranteeing MC are much tighter than bounds for AS.
%
\begin{lem} The ILC update law of the form $\sysInput_{k+1} = \sysInput_{k} - \lmatrix \error_{k}$ with $\lmatrix = \systemMat^{\dagger}$ operating on the unknown system with response $\error = \latentMat \sysInput + \linDist$ is monotonically convergent under the spectral norm with rate $1/\gamma$, $\gamma < 1$ if the matrices $\systemMat$, $\latentMat = \systemMat + \errorMat$ are full rank and $\|\errorMat\|_2 = \sigma_{\mathrm{max}}(\errorMat) < \gamma\sigma_{\mathrm{min}}^{2}(\systemMat)/(\mu\sigma_{\mathrm{max}}(\systemMat) + \mu\sigma_{\mathrm{min}}(\systemMat) + \gamma\sigma_{\mathrm{min}}(\systemMat))$, $\mu = \frac{1 + \sqrt{5}}{2}$.  \end{lem}
%
\begin{proof}
Writing again the dynamics equation and iterating with the ILC law we get
%
\begin{align}
&\error_{k+1} = \latentMat(\sysInput_k - \lmatrix\error_k) + \linDist, \\
&\error_{k+1} = \error_{k} - \latentMat\lmatrix\error_k = (\vec{I} - \latentMat\lmatrix)\error_k, \\
&\error_{k+1} - \error_{\infty} = (\vec{I} - \latentMat\lmatrix)\error_k - (\vec{I} - \latentMat(\lmatrix\latentMat)^{-1}\lmatrix)\error_k, \\
&\error_{k+1} - \error_{\infty} = (\latentMat(\lmatrix\latentMat)^{-1}\lmatrix - \latentMat\lmatrix)\error_k, \\
&\error_{k} - \error_{\infty} = \latentMat(\lmatrix\latentMat)^{-1}\lmatrix\error_k,
\end{align}
%
Since the system is AS according to Lemma~\ref{AS} and the inverse of $\lmatrix\latentMat$ exists, $\vec{I} = \lmatrix\latentMat(\lmatrix\latentMat)^{-1}$ and
%
\begin{align}
\|\error_{k+1} - \error_{\infty}\|_2 &= \|(\latentMat(\lmatrix\latentMat)^{-1}\lmatrix - \latentMat\lmatrix\latentMat(\lmatrix\latentMat)^{-1}\lmatrix)\error_k\|_2, \\
&= \|(\vec{I} - \latentMat\lmatrix)\latentMat(\lmatrix\latentMat)^{-1}\lmatrix\error_k\|_2, \\
&= \|(\vec{I} - \latentMat\latentMat^{\dagger})\latentMat(\lmatrix\latentMat)^{-1}\lmatrix\error_k \\
& - \latentMat(\lmatrix -\latentMat^{\dagger})\latentMat(\lmatrix\latentMat)^{-1}\lmatrix\error_k\|_2, \\
&= \|\latentMat(\systemMat^{\dagger} - \latentMat^{\dagger})\latentMat(\lmatrix\latentMat)^{-1}\lmatrix\error_k\|_2,
\end{align}
%
since $\latentMat\latentMat^{\dagger}\latentMat = \latentMat$. The matrix 2-norm is an induced norm and compatible by definition
%
\begin{align}
\|\error_{k+1} - \error_{\infty}\|_2 &\leq \|\latentMat\|_2\|\systemMat^{\dagger} - \latentMat^{\dagger}\|_2\|\error_{k} - \error_{\infty}\|_2,
%\|\error_{k+1} - \error_{\infty}\|_2 &< \gamma\|\error_{k} - \error_{\infty}\|_2,
\end{align}
%
We can use e.g. Theorem 4.1 in \cite{Wedin73} since rank$(\systemMat) = $ rank$(\latentMat)$ to bound the term containing the pseudoinverses
%
\begin{align}
\|\latentMat^{\dagger} - \systemMat^{\dagger}\|_{2} \leq \mu \|\systemMat^{\dagger}\|_2 \|\latentMat^{\dagger}\|_2 \|\errorMat\|_2,
\end{align}
%
further $\|\latentMat\|_2 \leq \|\systemMat\|_2 + \|\errorMat\|_2$, and bounding $\|\latentMat^{\dagger}\|_2$ with Lemma 3.1 in \cite{Wedin73} (because $\|\errorMat\|_2 < 1/\|\systemMat^{\dagger}\|_2$) we get
%
\begin{align}
\|\latentMat^{\dagger}\|_2 \leq \frac{\|\systemMat^{\dagger}\|_2}{1 - \|\systemMat^{\dagger}\|_2\|\errorMat\|_2}.
\end{align}
%
Putting it all together $\|\error_{k+1} - \error_{\infty}\|_2$ is less than or equal to
%
\begin{align}
& \frac{\mu \|\systemMat^{\dagger}\|^{2}\|\errorMat\|_2(\|\systemMat\|_2 + \|\errorMat\|_2)}{1 - \|\systemMat^{\dagger}\|_2\|\errorMat\|_2}\|\error_{k} - \error_{\infty}\|_2 \\
&<  \frac{\mu \|\systemMat^{\dagger}\|^{2}\|\errorMat\|_2(\|\systemMat\|_2 + 1/\|\systemMat^{\dagger}\|_2)}{1 - \|\systemMat^{\dagger}\|_2\|\errorMat\|_2}\|\error_{k+1} - \error_{\infty}\|_2.
\end{align}
%
Left hand side is less than $\gamma$ when
%
\begin{align}
\|\errorMat\|_2 &\leq \frac{\gamma}{\mu\|\systemMat^{\dagger}\|_2^{2}\|\systemMat\|_2 + \mu\|\systemMat^{\dagger}\|_2 + \gamma\|\systemMat^{\dagger}\|_2} \\
&= \frac{\gamma\sigma_{\mathrm{min}}^{2}(\systemMat)}{\mu\sigma_{\mathrm{max}}(\systemMat) + \mu\sigma_{\mathrm{min}}(\systemMat) + \gamma\sigma_{\mathrm{min}}(\systemMat)}.
\end{align}

%if $\sigma_{max}(\errorMat) < \frac{\gamma\sigma_{min}^{2}(\systemMat)}{\mu\sigma_{max}(\systemMat) + \mu\sigma_{min}(\systemMat) + \gamma\sigma_{min}(\systemMat)}$, as in Lemma~\ref{AS}.
%
\end{proof}
%
%Monotonically convergent ILC is by definition asymptotically stable. We would like to show that pseudoinverse-based ILC with model mismatch, i.e. $\lmatrix = \systemMat^{\dagger} = (\latentMat + \errorMat)^{\dagger}$ is still monotonically convergent for suitably bounded error matrices, e.g. $\|\errorMat\|_2 < \delta$ for some small $\delta \in \mathbb{R}$. Can these bounds be enlarged with TLS? The answer is yes.
%
%
% where does this fit in?
\subsection{Adaptive ILC based on Bayesian TLS}\label{adaptiveILC}

In this section we propose a natural Bayesian extension of the algorithm $\alg$ proposed in Section \ref{ilcTLS}. The resulting algorithm adapts the nominal models $\latentMat_k = \systemMat + \errorMat_k$ estimated with Bayesian linear regression. It is by definition a high-order ILC algorithm that considers all of the iterations, i.e all the deviations $\error_i, \ i = 1, \ldots, k,$ from the reference trajectory. 

We estimate the latent matrix and the disturbances using Bayesian linear regression, i.e.

\begin{align}
\begin{bmatrix} \sysInput_1^{\mathrm{T}} & 1 \\ \sysInput_2^{\mathrm{T}} & 1 \\ \vdots & \vdots \\ \sysInput_k^{\mathrm{T}} & 1 \end{bmatrix} \begin{bmatrix}
\latentMat^{\mathrm{T}} \\ \linDist^{\mathrm{T}}
\end{bmatrix} \approx \begin{bmatrix}
\error_1^{\mathrm{T}} \\ \vdots \\ \error_k^{\mathrm{T}}
\end{bmatrix}
\end{align}

where we incorporate our knowledge about the nominal dynamics with a prior on the parameter mean and  $\vec{\theta} = \begin{bmatrix}
\latentMat & \linDist \end{bmatrix}^{\mathrm{T}}$
%
\begin{align}
\rho(\vec{\theta},\vec{\Sigma}) = \rho(\vec{\Sigma})\rho(\vec{\theta}|\vec{\Sigma})
\end{align}
%
After this estimation procedure, at each iteration $k$ we can plug in the estimated matrix $\hat{\latentMat}$ and $\hat{\linDist}$ as well as the covariances $\vec{\Sigma} = \begin{bmatrix}
\vec{\Sigma}_Z & \vec{0} \\ \vec{0} & \vec{\Sigma}_d
\end{bmatrix}$ to weighted TLS and come up with $\sysInput_{k+1}$
%
\begin{equation}
\begin{aligned}
\text{minimize} &\ \| \hat{\vec{\Sigma}}_Z \begin{bmatrix} \errorMat_k & \residual_k \end{bmatrix} \hat{\vec{\Sigma}}_d \|, \\
\text{subject to} &\ -\hat{\linDist}_k + \residual_k \in \text{Range}(\hat{\latentMat}_k + \errorMat_k).
\end{aligned}
\end{equation}


%The prior weighting of the nominal model before starting the adaptive procedure can be based on the number of samples used in the identification of the model matrix $\systemMat$.

% Bayesian ILC is by definition a higher order ILC!