\section{Iterative Learning Control with Movement Primitives}\label{method}

In a complex task such as robot table tennis, one often needs to consider an extension of the standard trajectory tracking task. Based on the varying initial positions and velocities of the robot arm and the trajectory of the incoming ball, in each table tennis stroke the robot arm needs to track different trajectories that start from different initial conditions and end with different goal states of the arm. Moreover these trajectories need to be scaled in time to intercept the ball. Dynamic Movement Primitives (DMP) are especially useful for representing such a variety of movement patterns.
% reference needed? revise.

%Sometimes for safety reasons, for instance when interacting with external objects or under unforeseen perturbations \cite{Schaal07}, a \emph{low-gain} feedback law operating on the inputs may be fine-tuned to be compliant. As another practical restriction, one may not even be allowed to modify the low-level controller of the industrial robot \cite{Longman2000}. In such cases, it is not possible to modify the input signals $\sysInput$ directly. Instead one can modify the reference trajectories that are provided to the low-level controllers. It can be shown that this is an equivalent approach to modifying the feedforward control inputs \cite{Bristow06}.

Based on these considerations, in this paper we focus on learning to track DMPs $\dmp(t) = [\joint_{\text{des}}(t),\dot{\joint}_{\text{des}}(t)]^{\mathrm{T}}$. An initial DMP might be constructed out of a given demonstration or an optimal reference trajectory $\traj(t)$ using regression techniques \cite{Ijspeert13}. Representing a reference trajectory with movement primitives has some benefits: nonsmooth parts of the trajectory can be filtered, the evolution of desired states can be coupled with errors to ensure safety, time and scaling invariance of the differential equations can be used to adapt the trajectory to task changes in time as well as in space. %Robustness to initial joint position and velocity changes 

\subsection{Movement Primitive Formulation}
The dynamic movement primitive equations~\cite{Kober08} can be written as a weakly nonlinear system 
%
\begin{equation}
\begin{aligned}
\dot{\dmp} &= \begin{bmatrix}
   \dot{\dmp}_1 \\
   \dot{\dmp}_2
 \end{bmatrix} = \tau \begin{bmatrix}
     \dmp_2  \\
     \alpha_{g} (\beta_{g}(\goal - \dmp_1) - \dmp_2) +  \vec{\phi}(\phase)\weights
  \end{bmatrix}
\label{dmp},
\end{aligned}
\end{equation}
%
\noindent where the phase $\phase$ evolves according to
%
\begin{equation}
\dot{\phase} = -\tau\alpha_{\phase}\phase.
\label{phase}
\end{equation}
%
\noindent The constants $\tau$ and $\alpha_{\phase}$ determine the scaling and settling time respectively. 

The dynamical system \eqref{dmp} describes the motion of each of the desired joint states along a particular path in joint space. The forcing terms $\force = \vec{\phi}(\phase)\weights$ determine this path by warping the motion of the spring dynamics. The weights $\weights$ are obtained from regression using demonstration data. The spring constants $\alpha_{g}$ and $\beta_{g}$ ensure that starting from any initial position and velocity $\dmp_0$ the DMP converges to the goal state $\goal := \dmp_T = \traj_T$ and are usually chosen such that the dynamical system is critically damped.

% mention Jens' and Katharina's extension for hitting DMPs with desired velocity

% a figure showing convergence from different starting conditions needed here

\subsection{Derivation of ILC Updates}

Most ILC update laws can be put in the following form

\begin{equation}
\begin{aligned}
\sysInput_{k+1} = \qmatrix(\sysInput_{k} - \lmatrix\error_{k}).
\end{aligned}
\label{ILCupdateForm}
\end{equation}

\noindent Model based ILC can be cast in this form by stacking the model matrices in \eqref{discreteLTV} together to get the following lifted-vector representation \cite{Bristow06}, \cite{Schoellig12}

\begin{equation}
\begin{aligned}
\error_L &= \vec{F}\sysInput_L + \linDist_L, \\
\end{aligned}
\label{liftedLTV}
\end{equation}

\noindent where the submatrices of $\vec{F}$ are

\begin{equation*}
\begin{aligned}
\vec{F}_{(i,j)} &= \left \{
\begin{array}{cc}
\vec{A}_{i-1}\ldots \vec{A}_j \vec{B}_{j-1}, & j < i, \\ 
\vec{B}_{j-1}, & j = i, \\
\vec{0}, & j > i. 
\end{array} \right.
\end{aligned}
\end{equation*}

\noindent Using this \emph{input-to-output matrix} $\vec{F}$ we can analyze the effects of the feedforward inputs $\sysInput_L = [\linInput_1, \linInput_2, \ldots, \linInput_{\numSteps}]^{\mathrm{T}}$ on the errors $\error_L = [\error(1),\error(2),\ldots,\error(\numSteps)]^{\mathrm{T}}$ and compensate for the disturbances $\linDist_L$ with ILC.

%\noindent If the disturbances are repeating every iteration, i.e. $\frac{\partial{\linDist_L}}{\partial{\sysInput_L}} = 0$, using \eqref{liftedLTV},

\subsubsection{Lagrange form} The quadratic cost functional as the optimality criterion

\begin{equation}
\begin{aligned}
\ValueFunction(\state_0) &= \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\error + \linInput^{\mathrm{T}}\vec{R}\linInput \ \mathrm{d}t + \error_{T}^{\mathrm{T}}\vec{Q}_{T}\error_{T},
\end{aligned}
\label{cost}
\end{equation}

\noindent can be equally discretized and stacked in lifted vector form

\begin{equation}
\begin{aligned}
\ValueFunction_L &= \error_L^{\mathrm{T}}\vec{Q}_L\error_L + \sysInput_L^{\mathrm{T}}\vec{R}_L\sysInput_L,
\end{aligned}
\label{costFunctional}
\end{equation}

\noindent where $\vec{Q}_L \in \mathbb{R}^{2Nn \times 2Nn}$ (similarly for $\vec{R}_L$) is of the following form
%
\begin{equation*}
\begin{aligned}
 \vec{Q}_L &= 
 \begin{bmatrix}
  \vec{Q} & \vec{0} & \cdots & \vec{0} \\
  \vec{0} & \vec{Q} & \cdots & \vec{0} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \vec{0} & \vec{0} & \cdots & \vec{Q}_T
 \end{bmatrix}.
\end{aligned}
\end{equation*}

\noindent Using Gauss-Newton we can optimize iteratively for $\sysInput_L$

\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \Big(\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L}\Big)^{-1}\at{\frac{\partial{\ValueFunction_L}}{\partial{\sysInput_L}}}{\sysInput_k} \\
\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L} &= \frac{\partial}{\partial\sysInput_L}\{F^{\mathrm{T}}Q_L\error_L\} = F^{\mathrm{T}}Q_LF \\
\sysInput_{k+1} &= \sysInput_k - \beta_kF^{\dagger}\error_k
\end{aligned}
\label{ILCnewtonsMethod}
\end{equation}

\noindent ILC update law can be related to Newton's method if we consider the Hessian of the cost functional \eqref{cost2}
%
\begin{equation}
\begin{aligned}
\weights_{k+1} &= \weights_k - \beta_k\Big(\frac{\partial^{2}\ValueFunction}{\partial\weights^{2}}\Big)^{-1}\at{\frac{\partial{\ValueFunction}}{\partial{\weights}}}{\weights_k}, \\
\frac{\partial^{2}\ValueFunction}{\partial\weights^{2}} &= \frac{\partial}{\partial\weights}\{\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\error_L + \vec{R}_w\weights\} = \vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\vec{F}_{w} + \vec{R}_{\weights}, \\
\weights_{k+1} &= \qmatrix\weights_k - \beta_k(\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\vec{F}_{w} + \vec{R}_{w})^{-1}\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\error_k,
\end{aligned}
\label{ILCWeightsNewtonsMethod}
\end{equation}
%
\noindent where the filtering matrix is $\qmatrix = \beta_k(\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\vec{F}_{w} + \vec{R}_{w})^{-1}\vec{R}_{w}$. 

Note the connection of \eqref{ILCWeightsNewtonsMethod} to plant inversion methods~\cite{Bristow06}: taking $\vec{Q}_L = \vec{I}, \vec{R}_{w} = \vec{0}, \beta = 1,$ \eqref{ILCWeightsNewtonsMethod} becomes
%
\begin{equation}
\begin{aligned}
\weights_{k+1} &= \weights_k - \vec{F}_{w}^{\dagger}\error_k.
\end{aligned}
\label{ILCPlantInversion}
\end{equation}
%
% However this can be very unstable in practice, especially in nonminimum phase systems. Having a small, but nonzero weighting matrix R makes it much more stable.
%
\noindent Notice also the connection of \eqref{ILCRegression} with the online regression methods performed on the DMP weights \cite{Ijspeert13}. We can rewrite \eqref{ILCWeightsNewtonsMethod} as
%
\begin{equation}
\begin{aligned}
\weights_{k+1} &= \weights_k - \beta_k(\vec{\Psi}^{\mathrm{T}}\vec{W}\vec{\Psi} + \vec{R}_{w})^{-1}\vec{\Psi}^{\mathrm{T}}\vec{W}\error_k, \\
\vec{\Psi} &= \begin{bmatrix}
  \basis_1^{T} & \basis_2^{T} & \ldots & \basis_N^{T}
 \end{bmatrix}^{T}.
\end{aligned}
\label{ILCRegression}
\end{equation}
%
%
% online regression methods? are they discussed in the given reference?
In model-free approaches the weighting matrix $\vec{W}(t)$ is taken as the identity matrix. In our case the model-based assumptions of our approach appear in the form of a nondiagonal covariance matrix. We perform generalized least-squares regression by taking advantage of the linearized model in~\eqref{fullTransition} to form the right correlations between states. Compared with the \emph{credit-assignment} issues of RL algorithms, we see that ILC, equipped with our linearized models, offers us a more principled way to assign errors to the weights of the movement primitives.

\subsubsection{Mayer form}

\subsection{Iterative Learning Control for Movement Primitives}\label{ilcOnDMP} 

For the linearization of the robot dynamics \eqref{dynamics} under a given linear feedback law $\linInput(t) = -\vec{K}(t)(\state - \dmp)$ we consider the addition of the movement primitive dynamics to \eqref{LTV} 
%
\begin{equation}
\begin{aligned}
\dot{\fullvec} &= \vec{A}_{\fullvec}\fullvec(t) + \vec{B}_{\fullvec} \weights + \linDist(t,\weights),
\label{fullTransition}
\end{aligned}
\end{equation}
%
\noindent where for the enlarged vector $\fullvec = [\error,\dmp]^{\mathrm{T}}$ the transition matrices are 
%
\begin{IEEEeqnarray}{rCl}
%\footnotesize
\arraycolsep=3pt
\medmuskip = 1mu
\begin{aligned}
 \vec{A}_{\fullvec} &= \begin{bmatrix}
  \vec{A}(t) - \vec{B}(t)\vec{K}(t) & \vec{B}(t)\vec{K}(t) \\
  \vec{0} & \vec{A}_s
 \end{bmatrix}, \\
 \vec{B}_{\fullvec} &= \begin{bmatrix}
    \vec{0} \\
    \vec{\basis}(\phase)
   \end{bmatrix}. 
\end{aligned}
\label{fullMatrices}
\end{IEEEeqnarray}
%
\noindent The system matrices $\vec{A}$ and $\vec{B}$ ensure the coupling of the DMP to the states. 
%Reference trajectory $\traj(t)$ moves with a given velocity $\nu(t)$.

By detaching the reference trajectory $\traj(t)$ from the feedback law and hence from the dynamics \eqref{fullTransition} we can consider applying ILC techniques to the movement primitives $\dmp$. If we introduce the following cost functional as our optimality criterion
%
% We don't need the continous form!
%
\begin{IEEEeqnarray}{rCl}
\begin{aligned}
J(\weights) =& \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\error + \weights^{\mathrm{T}}\vec{R}_w \weights \ \mathrm{d}t \ \scalebox{0.9}[1]{\( + \)} \, (\state_T\scalebox{0.85}[1.0]{\( - \)}\goal)^{\mathrm{T}}\vec{Q}_{T}(\state_T\scalebox{0.85}[1.0]{\( - \)}\goal),
\end{aligned}
\label{cost2}
\end{IEEEeqnarray}
%
\noindent we can apply a weight-update form of the ILC update law~\cite{Bristow06}
%
\begin{equation}
\begin{aligned}
\weights_{k+1} = \qmatrix(\weights_{k} - \lmatrix\error_{k}),
\end{aligned}
\label{ILCupdateFormWeights}
\end{equation}
%
\noindent to minimize \eqref{cost2}. The index $k = 0, 1, \ldots$ denotes the iteration number. The matrices $\qmatrix$ and $\lmatrix$ are the filtering and learning matrices respectively~\cite{Bristow06}. They modulate the dependency between the inputs $\weights$ and the errors $\error$. Depending on these two matrices, the ILC update law \eqref{ILCupdateFormWeights} can be designed to be noncausal (i.e. predictive) and iteration-dependent. In order to reach the weight update law \eqref{ILCupdateFormWeights} we first discretize \eqref{cost2} and \eqref{fullTransition}. Then we stack the vectors together and get the following lifted-vector representation \cite{Bristow06}, \cite{Schoellig12}
%
\begin{equation}
\begin{aligned}
\ValueFunction &= \error_L^{\mathrm{T}}\vec{Q}_L\error_L + \weights^{\mathrm{T}}\vec{R}_{w}\weights,
\end{aligned}
\label{costFunctionalWeights}
\end{equation}


\subsection{Algorithm \& Implementation}\label{algorithm}

We use the update law derived in \eqref{ILCWeightsNewtonsMethod} in our algorithm, given in Algorithm~\ref{alg1}. This update law enables us to take advantage of the superlinear order of convergence property of Newton's method based descent methods while ensuring with $\qmatrix$ some degree of robustness on the nonrepeating disturbances in \eqref{fullTransition} arising from model mismatch and other unpredictable environmental conditions. Depending on the task and models available, weighting matrices $\vec{Q}_{L}$ and $\vec{R}_{w}$ can be varied as desired. Iteration dependent matrices will ensure that performance does not degrade, as one can reduce the weight matrix $\vec{R}_{w}$ as the errors get smaller. We discuss this additional degree of freedom of the algorithm in more detail in section \ref{experiments}.

%We initialize the algorithm $\alg$ with the necessary weighting and transition matrices. The transition matrix $A_{\fullvec}$ is derived by linearizing the nominal robot dynamics around the given reference trajectory $r$. Nominal inputs are acquired using the inverse dynamics model. Weights of the DMPs are initialized for each degree of freedom using regression on the reference trajectory.

% maybe reference needed
Notice the stability of the approach compared to iLQR methods where the reference trajectory $\traj(t)$ and the nominal inputs $\sysInput_{\mathrm{IDM}}$ are varying at each iteration, contributing to the source of the instabilities often encountered in such iterative optimal control approaches. However under high mismatch cases where the weight-to-output matrix $\vec{F}_{w}$ is not accurate, the descent direction might not be estimated and exploration in joint space might be necessary. We leave the extension of our work to such difficult settings for future work.

\begin{algorithm}[tb]
   \caption{\alg}
   \label{alg1}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\threshold > 0$, $\beta_k > 0$, $\vec{Q}_L, \vec{R}_{w} \succeq 0$, $\vec{A}_{\fullvec}$, $\ \traj = (\traj_1, \traj_2, \ldots, \traj_N)$, $\ \sysInput_{\mathrm{IDM}} = (\nu_1, \nu_2, \ldots, \nu_N)$
   \STATE Initialize $k = 1$, $\dmp(\weights) = \dmp(\weights_0)$
   \REPEAT 
 	   \STATE Run controller $\ddot{\joint} = \dynamics(\joint,\dot{\joint},\sysInput)$ %\sysInput = \nu - K_{\sysInput}(\state - \dmp(\weights)))$
 	   \STATE Observe $\error_k = \state_k - \traj$
 	   \STATE Compute $\ValueFunction_k$ = $\error_k^{\mathrm{T}}\vec{Q}_L\error_k + \weights^{\mathrm{T}}\vec{R}_w\weights$
 	   \STATE Form $\vec{F}_w$ using $\vec{A}_{\fullvec}$, $\vec{Q}_L$, $\vec{R}_{w}$
 	   \STATE Update $\weights \leftarrow \weights - \beta_k(\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\vec{F}_{w} + \vec{R}_{w})^{-1}\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\error_k$
 	   \STATE $k \leftarrow k + 1$
   \UNTIL{$\ValueFunction_k < \threshold$}
\end{algorithmic}
\end{algorithm}