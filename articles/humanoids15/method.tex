\section{Iterative Learning Control with Movement Primitives}\label{method}

In a complex task such as robot table tennis, one often needs to consider an extension of the standard trajectory tracking task. Based on the varying initial positions and velocities of the robot arm and the trajectory of the incoming ball, in each table tennis stroke the robot arm needs to track different trajectories that start from different initial conditions and end with different goal states of the arm. Moreover these trajectories need to be scaled in time to intercept the ball. Dynamic Movement Primitives (DMP) are especially useful for representing such a variety of movement patterns.
% reference needed? revise.

%Sometimes for safety reasons, for instance when interacting with external objects or under unforeseen perturbations \cite{Schaal07}, a \emph{low-gain} feedback law operating on the inputs may be fine-tuned to be compliant. As another practical restriction, one may not even be allowed to modify the low-level controller of the industrial robot \cite{Longman2000}. In such cases, it is not possible to modify the input signals $\sysInput$ directly. Instead one can modify the reference trajectories that are provided to the low-level controllers. It can be shown that this is an equivalent approach to modifying the feedforward control inputs \cite{Bristow06}.

Based on these considerations, in this paper we focus on learning to track DMPs $\dmp(t) = [\joint_{\text{des}}(t),\dot{\joint}_{\text{des}}(t)]^{\mathrm{T}}$. An initial DMP might be constructed out of a given demonstration or an optimal reference trajectory $\traj(t)$ using regression techniques \cite{Ijspeert13}. Representing a reference trajectory with movement primitives has some benefits: nonsmooth parts of the trajectory can be filtered, the evolution of desired states can be coupled with errors to ensure safety, time and scaling invariance of the differential equations can be used to adapt the trajectory to task changes in time as well as in space. %Robustness to initial joint position and velocity changes 

\subsection{Movement Primitive Formulation}
The dynamic movement primitive equations~\cite{Kober08} can be written as a weakly nonlinear system 
%
\begin{equation}
\begin{aligned}
\dot{\dmp} &= \begin{bmatrix}
   \dot{\dmp}_1 \\
   \dot{\dmp}_2
 \end{bmatrix} = \tau \begin{bmatrix}
     \dmp_2  \\
     \alpha_{g} (\beta_{g}(\goal - \dmp_1) - \dmp_2) +  \vec{\phi}(\phase)\weights
  \end{bmatrix}
\label{dmp},
\end{aligned}
\end{equation}
%
\noindent where the phase $\phase$ evolves according to
%
\begin{equation}
\dot{\phase} = -\tau\alpha_{\phase}\phase.
\label{phase}
\end{equation}
%
\noindent The constants $\tau$ and $\alpha_{\phase}$ determine the scaling and settling time respectively. 

The dynamical system \eqref{dmp} describes the motion of each of the desired joint states along a particular path in joint space. The forcing terms $\force = \vec{\phi}(\phase)\weights$ determine this path by warping the motion of the spring dynamics. The weights $\weights$ are obtained from regression using demonstration data. The spring constants $\alpha_{g}$ and $\beta_{g}$ ensure that starting from any initial position and velocity $\dmp_0$ the DMP converges to the goal state $\goal := \dmp_T = \traj_T$ and are usually chosen such that the dynamical system is critically damped.

% mention Jens' and Katharina's extension for hitting DMPs with desired velocity

% a figure showing convergence from different starting conditions needed here

\subsection{Derivation of ILC Updates}

Most ILC update laws can be put in the following form

\begin{equation}
\begin{aligned}
\sysInput_{k+1} = \qmatrix(\sysInput_{k} - \lmatrix\error_{k}).
\end{aligned}
\label{ILCupdateForm}
\end{equation}

\noindent Model based ILC can be cast in this form by stacking the model matrices in \eqref{discreteLTV} together to get the following lifted-vector representation \cite{Bristow06}, \cite{Schoellig12}

\begin{equation}
\begin{aligned}
\error_L &= \vec{F}\sysInput_L + \linDist_L, \\
\end{aligned}
\label{liftedLTV}
\end{equation}

\noindent where the submatrices of $\vec{F}$ are

\begin{equation*}
\begin{aligned}
\vec{F}_{(i,j)} &= \left \{
\begin{array}{cc}
\vec{A}_{i-1}\ldots \vec{A}_j \vec{B}_{j-1}, & j < i, \\ 
\vec{B}_{j-1}, & j = i, \\
\vec{0}, & j > i. 
\end{array} \right.
\end{aligned}
\end{equation*}

\noindent Using this \emph{input-to-output matrix} $\vec{F}$ we can analyze the effects of the feedforward inputs $\sysInput_L = \vect(\linInput)$ on the errors $\error_L = \vect(\error)$ and compensate for the disturbances $\linDist_L$ with ILC.

%\noindent If the disturbances are repeating every iteration, i.e. $\frac{\partial{\linDist_L}}{\partial{\sysInput_L}} = 0$, using \eqref{liftedLTV},

\subsubsection{Lagrange form} The quadratic cost functional as the optimality criterion

\begin{equation}
\begin{aligned}
\ValueFunction(\state_0) &= \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\error + \linInput^{\mathrm{T}}\vec{R}\linInput \ \mathrm{d}t + \error_{T}^{\mathrm{T}}\vec{Q}_{T}\error_{T},
\end{aligned}
\label{cost}
\end{equation}

\noindent can be equally discretized and stacked in lifted vector form

\begin{equation}
\begin{aligned}
\ValueFunction_L &= \error_L^{\mathrm{T}}\vec{Q}_L\error_L + \sysInput_L^{\mathrm{T}}\vec{R}_L\sysInput_L,
\end{aligned}
\label{costFunctional}
\end{equation}

\noindent where the symmetric positive definite matrix $\vec{Q}_L \in \mathbb{R}^{2Nn \times 2Nn}$ ($\vec{R}_L$ is defined analogously) is of the following form
%
\begin{equation*}
\begin{aligned}
 \vec{Q}_L &= 
 \begin{bmatrix}
  \vec{Q} & \vec{0} & \cdots & \vec{0} \\
  \vec{0} & \vec{Q} & \cdots & \vec{0} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \vec{0} & \vec{0} & \cdots & \vec{Q}_T
 \end{bmatrix}.
\end{aligned}
\end{equation*}
%
\noindent Using Newton's method we can optimize iteratively for $\sysInput_L$
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \Big(\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L}\Big)^{-1}\at{\frac{\partial{\ValueFunction_L}}{\partial{\sysInput_L}}}{\sysInput_k}, \\
\frac{1}{2}\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L} &= \frac{\partial}{\partial\sysInput_L}\{\vec{F}^{\mathrm{T}}\vec{Q}_L\error_L + \vec{R}_L\sysInput_L\} = \vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L, \\
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\error_k + \vec{R}_L\sysInput_k).
\end{aligned}
\label{ILCNewtonsMethod}
\end{equation}
%
\noindent Organizing \eqref{ILCNewtonsMethod} and comparing to \eqref{ILCupdateForm} we see that the filtering matrix $\qmatrix$ and the learning matrix $\lmatrix$ can be written as:
%
\begin{equation}
\begin{aligned}
\qmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F}),\\
\lmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F})^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L).
\end{aligned}
\label{qAndlMatrices}
\end{equation}
%
\noindent These matrices modulate the dependency between the inputs $\sysInput_k$ and the errors $\error_k$. Based on these two matrices, the ILC update law \eqref{ILCNewtonsMethod} is noncausal (i.e. predictive). See \cite{Amann95,Gunnarsson01} for the case where input derivative penalties are also considered. Note the connection of \eqref{ILCNewtonsMethod} to plant inversion methods~\cite{Bristow06}: taking $\vec{Q}_L = \vec{I}, \vec{R}_{L} = \vec{0},$ \eqref{ILCNewtonsMethod} becomes
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \vec{F}^{\dagger}\error_k.
\end{aligned}
\label{ILCPlantInversion}
\end{equation}
%
% However this can be very unstable in practice, especially in nonminimum phase systems. Having a small, but nonzero weighting matrix R makes it much more stable.
%
\noindent A more intuitive way to understand ILC is to consider \eqref{ILCPlantInversion} as least squares regression on the disturbances estimated using the previous trial
%
\begin{equation}
\begin{aligned}
\vec{F}\sysInput_{k+1} &\approx -\linDist_{k+1},\\
\sysInput_{k+1} &= \vec{F}^{\dagger}(\vec{F}\sysInput_{k} - \error_k),
\end{aligned}
\label{ILCasRegression}
\end{equation}
%
\noindent which is equivalent to \eqref{ILCPlantInversion} if $\vec{F}$ is of full column rank. We perform least-squares regression by taking advantage of the linearized model in~\eqref{discreteLTV} to form the right correlations between the errors and the feedforward compensations. Compared with the \emph{credit-assignment} issues of RL algorithms, we see that this brand of ILC, equipped with our linearized models for prediction, offers us a more principled way to assign errors to the control inputs.

% mention where the names of these forms are coming from
% are they appropriate? maybe give new names: final cost minimization
\subsubsection{Mayer form}

In some applications, the trajectory is only an intermediary and does not need to be precisely tracked. Hitting primitives and strokes in table tennis are examples of such a scenario, where the task performance depends only on reaching the goal state (i.e. desired position and velocity at the right time). In optimal control literature~\cite{Liberzon11} optimization criteria that consider only the final state cost are said to be in Mayer form, as opposed to the usual Lagrange form that considers the intermediate steps as well.

Taking the quadratic final cost, we can optimize it iteratively in a stable way by performing the transformation back to Lagrange form:
%
\begin{equation}
\begin{aligned}
\ValueFunction(\state_0) &= \frac{1}{2}\error_{T}^{\mathrm{T}}\vec{Q}\error_{T}, \\
&= \frac{1}{2}\error_{0}^{\mathrm{T}}\vec{Q}\error_{0} + \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\dot{\error} \, \mathrm{d}t.
\end{aligned}
\label{finalCost}
\end{equation}
%
\noindent which when discretized and stacked in lifted vector form can be written as 
%
\begin{equation}
\begin{aligned}
\ValueFunction_L &= \frac{1}{2}\error_{0}^{\mathrm{T}}\vec{Q}\error_{0} + \error_L^{\mathrm{T}}\vec{Q}_L\vec{D}\error_L.
\end{aligned}
\label{costFinalFunctional}
\end{equation}
%
\noindent where $\vec{D}$ is a stable difference operator, e.g.
%
\begin{equation*}
\begin{aligned}
 \vec{D} &= 
 \begin{bmatrix}
  -\vec{I} & \vec{I} & \vec{0} & \cdots & \vec{0} \\
  -\vec{I} & \vec{0} & \vec{I} & \cdots & \vec{0} \\
  \vdots  & \ddots  & \vdots & \ddots & \vdots  \\
  \vec{0} & \cdots & -\vec{I} & \vec{0} & \vec{I} \\
  \vec{0} & \cdots & \vec{0} & -\vec{I} & \vec{I}
 \end{bmatrix}.
\end{aligned}
\end{equation*}
%
\noindent Using Newton's method again we reach a different ILC update law
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}\error_k,\\
\vec{M} &= \vec{Q}_L \vec{D} + \vec{D}^{\mathrm{T}} \vec{Q}_L.
\end{aligned}
\label{ILCmayerForm}
\end{equation}


\subsection{Iterative Learning Control for Movement Primitives}\label{ilcOnDMP} 

Starting from different initial conditions $\state_0 = [\joint^{\intercal}_0,\dot{\joint}^{\intercal}_0]^{\mathrm{T}}$ we can consider a movement primitive as in \eqref{dmp} to give a smooth and feasible interpolating trajectory that is similar to the demonstrations.
For such movement primitives we consider the Mayer form ILC update \eqref{ILCmayerForm} to be the natural candidate since in each trial the DMPs generate different trajectories. Trying to track these varying trajectories with an ILC update law as in \eqref{ILCNewtonsMethod} can make learning unstable.




\subsection{Algorithm \& Implementation}\label{algorithm}

We use the update law derived in \eqref{ILCWeightsNewtonsMethod} in our algorithm, given in Algorithm~\ref{alg1}. This update law enables us to take advantage of the superlinear order of convergence property of Newton's method based descent methods while ensuring with $\qmatrix$ some degree of robustness on the nonrepeating disturbances in \eqref{fullTransition} arising from model mismatch and other unpredictable environmental conditions. Depending on the task and models available, weighting matrices $\vec{Q}_{L}$ and $\vec{R}_{w}$ can be varied as desired. Iteration dependent matrices will ensure that performance does not degrade, as one can reduce the weight matrix $\vec{R}_{w}$ as the errors get smaller. We discuss this additional degree of freedom of the algorithm in more detail in section \ref{experiments}.

%We initialize the algorithm $\alg$ with the necessary weighting and transition matrices. The transition matrix $A_{\fullvec}$ is derived by linearizing the nominal robot dynamics around the given reference trajectory $r$. Nominal inputs are acquired using the inverse dynamics model. Weights of the DMPs are initialized for each degree of freedom using regression on the reference trajectory.

% maybe reference needed
Notice the stability of the approach compared to iLQR methods where the reference trajectory $\traj(t)$ and the nominal inputs $\sysInput_{\mathrm{IDM}}$ are varying at each iteration, contributing to the source of the instabilities often encountered in such iterative optimal control approaches. However under high mismatch cases where the weight-to-output matrix $\vec{F}_{w}$ is not accurate, the descent direction might not be estimated and exploration in joint space might be necessary. We leave the extension of our work to such difficult settings for future work.

\begin{algorithm}[tb]
   \caption{\alg}
   \label{alg1}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\threshold > 0$, $\beta_k > 0$, $\vec{Q}_L, \vec{R}_{w} \succeq 0$, $\vec{A}_{\fullvec}$, $\ \traj = (\traj_1, \traj_2, \ldots, \traj_N)$, $\ \sysInput_{\mathrm{IDM}} = (\nu_1, \nu_2, \ldots, \nu_N)$
   \STATE Initialize $k = 1$, $\dmp(\weights) = \dmp(\weights_0)$
   \REPEAT 
 	   \STATE Run controller $\ddot{\joint} = \dynamics(\joint,\dot{\joint},\sysInput)$ %\sysInput = \nu - K_{\sysInput}(\state - \dmp(\weights)))$
 	   \STATE Observe $\error_k = \state_k - \traj$
 	   \STATE Compute $\ValueFunction_k$ = $\error_k^{\mathrm{T}}\vec{Q}_L\error_k + \weights^{\mathrm{T}}\vec{R}_w\weights$
 	   \STATE Form $\vec{F}_w$ using $\vec{A}_{\fullvec}$, $\vec{Q}_L$, $\vec{R}_{w}$
 	   \STATE Update $\weights \leftarrow \weights - \beta_k(\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\vec{F}_{w} + \vec{R}_{w})^{-1}\vec{F}_{w}^{\mathrm{T}}\vec{Q}_L\error_k$
 	   \STATE $k \leftarrow k + 1$
   \UNTIL{$\ValueFunction_k < \threshold$}
\end{algorithmic}
\end{algorithm}