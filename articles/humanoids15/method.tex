\section{Iterative Learning Control with Movement Primitives}\label{method}

In a complex task such as robot table tennis, one often needs to consider an extension of the standard trajectory tracking task. Based on the varying initial positions and velocities of the robot arm and the trajectory of the incoming ball, in each table tennis stroke the robot arm needs to track different trajectories that start from different initial conditions and end with different goal states of the arm. Moreover these trajectories need to be scaled in time to intercept the ball. Dynamic Movement Primitives (DMP) are especially useful for representing such a variety of movement patterns.
% reference needed? revise.

%Sometimes for safety reasons, for instance when interacting with external objects or under unforeseen perturbations \cite{Schaal07}, a \emph{low-gain} feedback law operating on the inputs may be fine-tuned to be compliant. As another practical restriction, one may not even be allowed to modify the low-level controller of the industrial robot \cite{Longman2000}. In such cases, it is not possible to modify the input signals $\sysInput$ directly. Instead one can modify the reference trajectories that are provided to the low-level controllers. It can be shown that this is an equivalent approach to modifying the feedforward control inputs \cite{Bristow06}.

Based on these considerations, in this paper we focus on learning to track DMPs $\vec{\dmp}(t) = [\joint_{\text{des}}(t),\dot{\joint}_{\text{des}}(t)]^{\mathrm{T}}$. An initial DMP might be constructed out of a given demonstration or an optimal reference trajectory $\traj(t)$ using regression techniques \cite{Ijspeert13}. Representing a reference trajectory with movement primitives has some benefits: nonsmooth parts of the trajectory can be filtered, the evolution of desired states can be coupled with errors to ensure safety, time and scaling invariance of the differential equations can be used to adapt the trajectory to task changes in time as well as in space. %Robustness to initial joint position and velocity changes 

\subsection{Movement Primitive Formulation}

Striking movement primitives suited to table tennis have been proposed in \cite{Kober10} and \cite{Muelling13} as an extension of discrete DMPs. Unlike the original formulation~\cite{Ijspeert02}, these extensions allow for an arbitrary velocity profile to be attached to the primitives around hitting time. However, these \emph{ad hoc} approaches are unnecessarily cluttered and involve additional tuning parameters. Arguably they are also against the philosophy behind the discrete primitive: the motion converges necessarily to a goal state with zero velocity. We propose here instead to use rhythmic DMPs that allow for a limit cycle attractor, which is inevitable if we want to maintain the striking motion through goal state. After the striking is completed the DMP can be used to return back to initial state or it can be terminated by setting the forcing terms to zero.

The rhythmic movement primitive equations~\cite{Kober08} can each be written as
%
\begin{equation}
\begin{aligned}
\begin{bmatrix}
   \dot{\dmp}_1 \\
   \dot{\dmp}_2
 \end{bmatrix} = \tau \begin{bmatrix}
     \dmp_2  \\
     \alpha_{g} (\beta_{g}(\goal - \dmp_1) - \dmp_2) +  \force(\phase)
  \end{bmatrix}
\label{dmp},
\end{aligned}
\end{equation}
%
\noindent where the phase $\phase$ evolves as $\dot{\phase} = \tau$. The constant $\tau$ determines the period of the limit cycle and the forcing term $\force$ enforces the limit cycle with $M$ weighted Von-Mises basis functions $\basis_i$
%
\begin{equation}
\begin{aligned}
\force(\phase,\amp) &= \frac{\sum_{i=1}^{M}\basis_i \weights_i}{\sum_{i=1}^{M}\basis_i}\amp, \\
\basis_i &= \exp(\basisHeight_i (\cos(\phase - \basisCenter_i) - 1)).
\label{forcing}
\end{aligned}
\end{equation}
%
\noindent Here $\amp, \basisHeight_i, \basisCenter_i$ determine the amplitude of the motion, the width and the centers of the basis functions respectively.

The dynamical system \eqref{dmp} describes the motion of each of the desired joint states along a particular periodic path in joint space. The forcing terms determine this path by warping the motion of the spring dynamics. The weights $\weights$ are obtained with regression using demonstration data. The spring constants $\alpha_{g}$ and $\beta_{g}$ ensure that starting from any initial position and velocity $\dmp_0$ the DMP converges to the limit cycle with center $\goal$ and are usually chosen such that the dynamical system is critically damped.

Unlike the discrete DMP, the rhythmic DMP is a linear time-varying system and adaptions of target positions and velocities can be easily performed. Using linear systems theory we prove the feasibility of such adaptations and construct the necessary modifications of the DMP parameters $\goal,\amp$ in the Appendix.

% a figure showing convergence from different starting conditions needed here

\subsection{Derivation of ILC Updates}

Most ILC update laws can be put in the following form

\begin{equation}
\begin{aligned}
\sysInput_{k+1} = \qmatrix(\sysInput_{k} - \lmatrix\error_{k}).
\end{aligned}
\label{ILCupdateForm}
\end{equation}

\noindent Model based ILC can be cast in this form by stacking the model matrices in \eqref{discreteLTV} together to get the following lifted-vector representation \cite{Bristow06}, \cite{Schoellig12}

\begin{equation}
\begin{aligned}
\error_L &= \vec{F}\sysInput_L + \linDist_L, \\
\end{aligned}
\label{liftedLTV}
\end{equation}

\noindent where the submatrices of $\vec{F}$ are

\begin{equation}
\begin{aligned}
\vec{F}_{(i,j)} &= \left \{
\begin{array}{cc}
\vec{A}_{i-1}\ldots \vec{A}_j \vec{B}_{j-1}, & j < i, \\ 
\vec{B}_{j-1}, & j = i, \\
\vec{0}, & j > i. 
\end{array} \right.
\end{aligned}
\label{Fmatrix}
\end{equation}

\noindent Using this \emph{input-to-output matrix} $\vec{F}$ we can analyze the effects of the feedforward inputs $\sysInput_L = \vect(\linInput)$ on the errors $\error_L = \vect(\error)$ and compensate for the disturbances $\linDist_L$ with ILC.

%\noindent If the disturbances are repeating every iteration, i.e. $\frac{\partial{\linDist_L}}{\partial{\sysInput_L}} = 0$, using \eqref{liftedLTV},

\subsubsection{Lagrange form} The quadratic cost functional as the optimality criterion

\begin{equation}
\begin{aligned}
\ValueFunction(\linInput) &= \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\error + \linInput^{\mathrm{T}}\vec{R}\linInput \ \mathrm{d}t + \error_{T}^{\mathrm{T}}\vec{Q}_{T}\error_{T},
\end{aligned}
\label{cost}
\end{equation}

\noindent can be equally discretized and stacked in lifted vector form

\begin{equation}
\begin{aligned}
\ValueFunction_L &= \error_L^{\mathrm{T}}\vec{Q}_L\error_L + \sysInput_L^{\mathrm{T}}\vec{R}_L\sysInput_L,
\end{aligned}
\label{costFunctional}
\end{equation}

\noindent where the symmetric positive definite matrix $\vec{Q}_L \in \mathbb{R}^{2Nn \times 2Nn}$ ($\vec{R}_L$ is defined analogously) is of the following form
%
\begin{equation}
\begin{aligned}
 \vec{Q}_L &= 
 \begin{bmatrix}
  \vec{Q} & \vec{0} & \cdots & \vec{0} \\
  \vec{0} & \vec{Q} & \cdots & \vec{0} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \vec{0} & \vec{0} & \cdots & \vec{Q}_T
 \end{bmatrix}.
\end{aligned}
\label{Qmatrix}
\end{equation}
%
\noindent Using Newton's method we can optimize iteratively for $\sysInput_L$
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \Big(\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L}\Big)^{-1}\at{\frac{\partial{\ValueFunction_L}}{\partial{\sysInput_L}}}{\sysInput_k}, \\
\frac{1}{2}\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L} &= \frac{\partial}{\partial\sysInput_L}\{\vec{F}^{\mathrm{T}}\vec{Q}_L\error_L + \vec{R}_L\sysInput_L\} = \vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L, \\
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\error_k + \vec{R}_L\sysInput_k).
\end{aligned}
\label{ILCNewtonsMethod}
\end{equation}
%
\noindent Organizing \eqref{ILCNewtonsMethod} and comparing to \eqref{ILCupdateForm} we see that the filtering matrix $\qmatrix$ and the learning matrix $\lmatrix$ can be written as:
%
\begin{equation}
\begin{aligned}
\qmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F}),\\
\lmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F})^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L).
\end{aligned}
\label{qAndlMatrices}
\end{equation}
%
\noindent These matrices modulate the dependency between the inputs $\sysInput_k$ and the errors $\error_k$. Based on these two matrices, the ILC update law \eqref{ILCNewtonsMethod} is noncausal (i.e. predictive). See \cite{Amann95,Gunnarsson01} for the case where input derivative penalties are also considered. Note the connection of \eqref{ILCNewtonsMethod} to plant inversion methods~\cite{Bristow06}: taking $\vec{Q}_L = \vec{I}, \vec{R}_{L} = \vec{0},$ \eqref{ILCNewtonsMethod} becomes
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \vec{F}^{\dagger}\error_k.
\end{aligned}
\label{ILCPlantInversion}
\end{equation}
%
% However this can be very unstable in practice, especially in nonminimum phase systems. Having a small, but nonzero weighting matrix R makes it much more stable.
%
\noindent A more intuitive way to understand ILC is to consider \eqref{ILCPlantInversion} as least squares regression on the disturbances estimated using the previous trial
%
\begin{equation}
\begin{aligned}
\vec{F}\sysInput_{k+1} &\approx -\linDist_{k},\\
\sysInput_{k+1} &= \vec{F}^{\dagger}(\vec{F}\sysInput_{k} - \error_k),
\end{aligned}
\label{ILCasRegression}
\end{equation}
%
\noindent which is equivalent to \eqref{ILCPlantInversion} if $\vec{F}$ is of full column rank. We perform least-squares regression by taking advantage of the linearized model in~\eqref{discreteLTV} to form the right correlations between the errors and the feedforward compensations. Compared with the \emph{credit-assignment} issues of RL algorithms, we see that this brand of ILC, equipped with our linearized models for prediction, offers us a more principled way to assign errors to the control inputs.

% mention where the names of these forms are coming from
% are they appropriate? maybe give new names: final cost minimization
\subsubsection{Mayer form}

In some applications, the trajectory is only an intermediary and does not need to be precisely tracked. Hitting primitives and strokes in table tennis are examples of such a scenario, where the task performance depends only on reaching the desired position and velocity at the right time. In optimal control literature~\cite{Liberzon11} optimization criteria that consider only the final state cost are said to be in Mayer form, as opposed to the usual Lagrange form that considers the intermediate steps as well.

Taking the quadratic final cost, we can relate it to the feedforward corrections $\sysInput_L$ using our linearized model \eqref{liftedLTV}
%
\begin{equation}
\begin{aligned}
\ValueFunction(\sysInput_L) &= \error_{N}^{\mathrm{T}}\vec{Q}\error_{N}, \\
&= (\vec{F}_N\sysInput_L + \linDist_N)^{\mathrm{T}}\vec{Q}(\vec{F}_N\sysInput_L + \linDist_N).
\end{aligned}
\label{finalCost}
\end{equation}
%
\noindent where $\vec{F}_N$ is the last block row of $\vec{F}$ in \eqref{Fmatrix}
%
\begin{equation*}
\begin{aligned}
 \vec{F}_N &= 
 \begin{bmatrix}
  \vec{A}_{N-1} \ldots \vec{A}_2 \vec{B}_1 & \cdots & \vec{B}_N 
 \end{bmatrix}.
\end{aligned}
\end{equation*}
%
\noindent Using Newton's method again we reach the reweighted form of \eqref{ILCPlantInversion}
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}\error_k,\\
\end{aligned}
\label{ILCmayerForm}
\end{equation}
%
\noindent here $\vec{M}$ is \eqref{Qmatrix} with $\vec{Q} = \vec{0}$.


\subsection{Iterative Learning Control for Movement Primitives}\label{ilcOnDMP} 

Execution errors in tracking the rhythmic DMP prevents us from initializing the robot at each iteration to the same state. Starting from different initial conditions $\state_0 = [\joint^{\intercal}_0,\dot{\joint}^{\intercal}_0]^{\mathrm{T}}$ we can consider a movement primitive as in \eqref{dmp} to give a smooth and feasible interpolating trajectory that is similar to the demonstrations.
For such movement primitives we consider the Mayer form ILC update \eqref{ILCmayerForm} to be the natural candidate since in each trial the DMPs generate different trajectories. Trying to track these varying trajectories with an ILC update law as in \eqref{ILCNewtonsMethod} can make learning unstable.

To compensate for the varying initial conditions we make a correction in the reference control input $\trjInput$ based on the nominal inverse dynamics model. For additional stability we consider the effects of the initial errors on the final error by adjusting our linearized model
%
\begin{equation}
\begin{aligned}
\error_N &= \vec{F}_N\sysInput_L + \vec{H}_N\error_0 + \linDist_N, \\
\end{aligned}
\label{adjustedLiftedLTV}
\end{equation}
%
\noindent where $\vec{H}_N = \begin{bmatrix} \vec{A}_{N-1} \ldots \vec{A}_2 \vec{A}_1 \end{bmatrix}$ is the last block row of the free response $\vec{H}$ to the initial error $\error_0$. With this correction the regression model \eqref{ILCasRegression} takes a particular form
%
\begin{equation}
\begin{aligned}
\vec{F}\sysInput_{k+1} &= -\linDist_{k+1} - \vec{H}\vec{\delta}_{k+1},\\
\vec{F}\sysInput_{k+1} &\approx \vec{F}\sysInput_{k} - \error_k + \vec{H}\vec{\delta}_{k} - \vec{H}\vec{\delta}_{k+1}.
\end{aligned}
\label{ILCregressionWithe0}
\end{equation}
%
\noindent Here the initial errors are denoted as $\vec{\delta}_k$ to emphasize its iteration dependence. Combining \eqref{ILCregressionWithe0} with \eqref{ILCmayerForm} we get the final form of the ILC update
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}(\error_k + \vec{H}(\vec{\delta}_{k} - \vec{\delta}_{k+1})).\\
\end{aligned}
\label{ILCfinalForm}
\end{equation}

\subsection{Algorithm \& Implementation}\label{algorithm}

We use the update law derived in \eqref{ILCfinalForm} in our algorithm, given in Algorithm~\ref{alg1}. This update law enables us to take advantage of the superlinear order of convergence property of Newton's method based descent methods while ensuring robustness on the variations in initial conditions. Depending on the task and models available, weighting matrices $\vec{M}$ and $\vec{R}$ can be varied as desired. Iteration dependent matrices will ensure that performance does not degrade, as one can reduce the weight matrix $\vec{R}$ as the errors get smaller. $\vec{M}$ can also be made inversely proportional to the size of the errors in tracking the movement primitive. %We discuss the additional degrees of freedom of the algorithm in more detail in section \ref{experiments}.

%We initialize the algorithm $\alg$ with the necessary weighting and transition matrices. The transition matrix $A_{\fullvec}$ is derived by linearizing the nominal robot dynamics around the given reference trajectory $r$. Nominal inputs are acquired using the inverse dynamics model. Weights of the DMPs are initialized for each degree of freedom using regression on the reference trajectory.

% maybe reference needed
Notice the stability of the approach compared to iLQR methods where the reference trajectory $\traj(t)$ and the nominal inputs $\sysInput_{\mathrm{IDM}}$ are varying at each iteration, contributing to the source of the instabilities often encountered in such iterative optimal control approaches. However under high mismatch cases where the input-to-output matrix $\vec{F}$ is not accurate, the descent direction might not be estimated and exploration in joint space might be necessary. We leave the extension of ILC to such adaptive settings for future work.

\begin{algorithm}[tb]
   \caption{\alg}
   \label{alg1}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\threshold > 0$, $\vec{Q}, \vec{R} \succeq 0$, $\vec{\dmp}$ 
   \STATE Form $\vec{F}$ using $\vec{A}$, $\vec{B}$ matrices
   \STATE Initialize $k = 1$, $\ilcInput = \vec{0}$
   \REPEAT 
   	   \STATE Rollout $\vec{\dmp}$ from $\state_0 + \vec{\delta}_k$ 
   	   \STATE Compute $\trjInput$ with inverse dynamics
 	   \STATE Strike with controls $\sysInput = \trjInput + \ilcInput$  %\sysInput = \nu - K_{\sysInput}(\state - \dmp(\weights)))$
 	   \STATE Observe $\error_k = \state_k - \dmp$ from $\ddot{\joint} = \dynamics(\joint,\dot{\joint},\sysInput)$
 	   \STATE Compute $\ValueFunction_k$ = $\error_N^{\mathrm{T}}\vec{Q}\error_N$
 	   \STATE Update $\ilcInput \leftarrow \ilcInput - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}(\error_k + \vec{H}(\vec{\delta}_k - \vec{\delta}_{k+1}))$
 	   \STATE Follow $\vec{\dmp}$ to $\state_0$
   \UNTIL{$\ValueFunction_k < \threshold$}
\end{algorithmic}
\end{algorithm}