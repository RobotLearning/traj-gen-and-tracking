\section{Iterative Learning Control with Movement Primitives}\label{method}

In a highly dynamic and complex task such as robot table tennis, one often needs to consider an extension of the standard trajectory tracking task. Based on the varying initial positions and velocities of the robot arm and the trajectory of the incoming ball, in each table tennis stroke the robot arm needs to track different trajectories that start from different initial conditions and end with different goal states of the arm. Moreover these trajectories need to be scaled in time to intercept the ball. Dynamic Movement Primitives (DMP) are especially useful for representing such a variety of movement patterns.
% reference needed? revise.

%Sometimes for safety reasons, for instance when interacting with external objects or under unforeseen perturbations \cite{Schaal07}, a \emph{low-gain} feedback law operating on the inputs may be fine-tuned to be compliant. As another practical restriction, one may not even be allowed to modify the low-level controller of the industrial robot \cite{Longman2000}. In such cases, it is not possible to modify the input signals $\sysInput$ directly. Instead one can modify the reference trajectories that are provided to the low-level controllers. It can be shown that this is an equivalent approach to modifying the feedforward control inputs \cite{Bristow06}.

Based on these considerations, in this paper we focus on learning to track DMPs $\vec{\dmp}(t) = [\joint_{\text{des}}(t),\dot{\joint}_{\text{des}}(t)]^{\mathrm{T}}$. An initial DMP might be constructed out of a given demonstration or an optimal reference trajectory $\traj(t)$ using regression techniques \cite{Ijspeert13}. Representing a reference trajectory with movement primitives has some benefits: nonsmooth parts of the trajectory can be filtered, the evolution of desired states can be coupled with errors to ensure safety, time and scaling invariance of the differential equations can be used to adapt the trajectory to task changes in time as well as in space. %Robustness to initial joint position and velocity changes 

\subsection{Movement Primitive Formulation}

Striking movement primitives suited to table tennis have been proposed in \cite{Kober10} and \cite{Muelling13} as an extension of discrete DMPs. Unlike the original formulation~\cite{Ijspeert02}, these extensions allow for an arbitrary velocity profile to be attached to the primitives around hitting time. However, these \emph{ad hoc} approaches are unnecessarily cluttered and involve additional tuning parameters. Arguably they are also against the philosophy behind the discrete primitive: the motion converges necessarily to a goal state with zero velocity. This can not only lead to potentially unsafe states at the completion of the primitive execution, but also requires the addition design of a returning trajectory.

We propose here instead to use rhythmic DMPs that allow for a limit cycle attractor, which is inevitable if we want to maintain the striking motion through goal state. After the striking is completed the DMP can be used to return back to initial state or it can be terminated by setting the forcing terms to zero. An example is shown in Figure~\ref{rdmp}.

\begin{figure}[b!]
\center
\includegraphics[scale=0.30]{rdmp.pdf}
%\scalebox{1.0}{\input{Pictures/ilc.tikz}}
\caption{An example of a striking movement primitive is shown in red. The rhythmic DMP creates a limiting cycle in the workspace of the robot. Executing this movement well will lead to a good hit. Control errors in tracking lead to a poor hitting performance, shown in blue. The tracking errors can be decreased efficiently by applying model-based iterative learning updates.} 
\label{rdmp}
\end{figure}

The rhythmic movement primitive differential equations~\cite{Kober08} can be rewritten as
%
\begin{equation}
\begin{aligned}
\begin{bmatrix}
   \dot{\dmp}_1 \\
   \dot{\dmp}_2
 \end{bmatrix} = \begin{bmatrix}
     0 & 1  \\
     -\tau^2 \alpha_{g} \beta_{g} & - \tau \alpha_{g}  
  \end{bmatrix}\begin{bmatrix}
     \dmp_1 \\
     \dmp_2
   \end{bmatrix} + 
\tau^{2}\begin{bmatrix}
0 & 0 \\
\alpha_{g} \beta_{g} & \force(\phase)
\end{bmatrix} \begin{bmatrix}
g \\ A
\end{bmatrix}
\label{dmp},
\end{aligned}
\end{equation}
%
\noindent where the phase $\phase$ evolves as $\dot{\phase} = \tau$. The constant $\tau$ determines the period of the limit cycle and the forcing term $\force$ enforces the limit cycle with $M$ weighted Von-Mises basis functions $\basis_i$
%
\begin{equation}
\begin{aligned}
\force(\phase) &= \frac{\sum_{i=1}^{M}\basis_i \weights_i}{\sum_{i=1}^{M}\basis_i}, \\
\basis_i &= \exp(\basisHeight_i (\cos(\phase - \basisCenter_i) - 1)).
\label{forcing}
\end{aligned}
\end{equation}
%
\noindent Here $\amp, \basisHeight_i, \basisCenter_i$ determine the amplitude of the motion, the width and the centers of the basis functions respectively.

The dynamical system \eqref{dmp} describes the motion of each of the desired joint states along a particular periodic path in joint space. The forcing term shapes this path by warping the motion of the spring dynamics. The weights $\weights$ are obtained with regression using demonstration data. The spring constants $\alpha_{g}$ and $\beta_{g}$ ensure that starting from any initial position and velocity $\dmp_0$ the DMP converges to the limit cycle with center $\goal$ and are usually chosen such that the dynamical system is critically damped.

Unlike the discrete DMP, the rhythmic DMP is a linear time-varying system and adaptions of target positions and velocities can be easily performed. Using linear systems theory one can show the feasibility of such adaptations and construct phase-dependent modifications of the free parameters $\goal,\amp$.

\subsection{Derivation of ILC Updates}

Most ILC update laws can be put in the following form

\begin{equation}
\begin{aligned}
\sysInput_{k+1} = \qmatrix(\sysInput_{k} - \lmatrix\error_{k}).
\end{aligned}
\label{ILCupdateForm}
\end{equation}

\noindent Model based ILC can be cast in this form by stacking the model matrices in \eqref{discreteLTV} together to get the following lifted-vector representation \cite{Bristow06}, \cite{Schoellig12}

\begin{equation}
\begin{aligned}
\error_L &= \vec{F}\sysInput_L + \linDist_L, \\
\end{aligned}
\label{liftedLTV}
\end{equation}

\noindent where the submatrices of $\vec{F}$ are

\begin{equation}
\begin{aligned}
\vec{F}_{(i,j)} &= \left \{
\begin{array}{cc}
\vec{A}_{i-1}\ldots \vec{A}_j \vec{B}_{j-1}, & j < i, \\ 
\vec{B}_{j-1}, & j = i, \\
\vec{0}, & j > i. 
\end{array} \right.
\end{aligned}
\label{Fmatrix}
\end{equation}

\noindent Using this \emph{input-to-output matrix} $\vec{F}$ we can analyze the effects of the feedforward inputs $\sysInput_L = \vect(\linInput)$ on the errors $\error_L = \vect(\error)$ and compensate for the disturbances $\linDist_L$ with ILC.

%\noindent If the disturbances are repeating every iteration, i.e. $\frac{\partial{\linDist_L}}{\partial{\sysInput_L}} = 0$, using \eqref{liftedLTV},

\subsubsection{Lagrange form} The quadratic cost functional as the optimality criterion

\begin{equation}
\begin{aligned}
\ValueFunction(\linInput) &= \int_{0}^{T} \error^{\mathrm{T}}\vec{Q}\error + \linInput^{\mathrm{T}}\vec{R}\linInput \ \mathrm{d}t + \error_{T}^{\mathrm{T}}\vec{Q}_{T}\error_{T},
\end{aligned}
\label{cost}
\end{equation}

\noindent can be equally discretized and stacked in lifted vector form

\begin{equation}
\begin{aligned}
\ValueFunction_L &= \error_L^{\mathrm{T}}\vec{Q}_L\error_L + \sysInput_L^{\mathrm{T}}\vec{R}_L\sysInput_L,
\end{aligned}
\label{costFunctional}
\end{equation}

\noindent where the symmetric positive definite matrix $\vec{Q}_L \in \mathbb{R}^{2Nn \times 2Nn}$ ($\vec{R}_L$ is defined analogously) is of the following form
%
\begin{equation}
\begin{aligned}
 \vec{Q}_L &= 
 \begin{bmatrix}
  \vec{Q}_1 & \vec{0} & \cdots & \vec{0} \\
  \vec{0} & \vec{Q}_2 & \cdots & \vec{0} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  \vec{0} & \vec{0} & \cdots & \vec{Q}_N
 \end{bmatrix}.
\end{aligned}
\label{Qmatrix}
\end{equation}
%
\noindent Using Newton's method we can optimize iteratively for $\sysInput_L$
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \Big(\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L}\Big)^{-1}\at{\frac{\partial{\ValueFunction_L}}{\partial{\sysInput_L}}}{\sysInput_k}, \\
\frac{1}{2}\frac{\partial^{2}\ValueFunction_L}{\partial\sysInput^{2}_L} &= \frac{\partial}{\partial\sysInput_L}\{\vec{F}^{\mathrm{T}}\vec{Q}_L\error_L + \vec{R}_L\sysInput_L\} = \vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L, \\
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\error_k + \vec{R}_L\sysInput_k).
\end{aligned}
\label{ILCNewtonsMethod}
\end{equation}
%
\noindent The update \eqref{ILCNewtonsMethod} will be referred to as the Newton-based update, or \emph{N-ILC}. Organizing \eqref{ILCNewtonsMethod} and comparing to \eqref{ILCupdateForm} we see that the filtering matrix $\qmatrix$ and the learning matrix $\lmatrix$ can be written as:
%
\begin{equation}
\begin{aligned}
\qmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F} + \vec{R}_L)^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F}),\\
\lmatrix &= (\vec{F}^{\mathrm{T}}\vec{Q}_L\vec{F})^{-1}(\vec{F}^{\mathrm{T}}\vec{Q}_L).
\end{aligned}
\label{qAndlMatrices}
\end{equation}
%
\noindent These matrices modulate the dependency between the inputs $\sysInput_k$ and the errors $\error_k$. The ILC update law \eqref{ILCNewtonsMethod} is noncausal (i.e. predictive) since $\lmatrix$ is not block lower-diagonal. See \cite{Amann95,Gunnarsson01} for the case where input derivative penalties are also considered. Note the connection of \eqref{ILCNewtonsMethod} to plant inversion methods~\cite{Bristow06}: taking $\vec{Q}_L = \vec{I}, \vec{R}_{L} = \vec{0},$ \eqref{ILCNewtonsMethod} becomes
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - \vec{F}^{\dagger}\error_k.
\end{aligned}
\label{ILCPlantInversion}
\end{equation}
%
% However this can be very unstable in practice, especially in nonminimum phase systems. Having a small, but nonzero weighting matrix R makes it much more stable.
%
\noindent A more intuitive way to understand ILC is to consider \eqref{ILCPlantInversion} as least squares regression on the disturbances estimated using the previous trial
%
\begin{equation}
\begin{aligned}
\vec{F}\sysInput_{k+1} &\approx -\linDist_{k},\\
\sysInput_{k+1} &= \vec{F}^{\dagger}(\vec{F}\sysInput_{k} - \error_k),
\end{aligned}
\label{ILCasRegression}
\end{equation}
%
\noindent which is equivalent to \eqref{ILCPlantInversion} if $\vec{F}$ is of full column rank. We perform least-squares regression by taking advantage of the linearized model in~\eqref{discreteLTV} to form the right correlations between the errors and the feedforward compensations. Compared with the \emph{credit-assignment} issues of RL algorithms, we see that this brand of ILC, equipped with our linearized models for prediction, offers us a more principled way to assign errors to the control inputs.

% mention where the names of these forms are coming from
% are they appropriate? maybe give new names: final cost minimization
\subsubsection{Mayer form}

In some applications, the trajectory is only an intermediary and does not need to be precisely tracked. Hitting primitives and strokes in table tennis are examples of such a scenario, where the task performance depends only on reaching the desired position and velocity at the right time. In optimal control literature optimization criteria that consider only the final state cost are said to be in Mayer form~\cite{Liberzon11}, as opposed to the usual Lagrange form that considers the intermediate steps as well.

Taking the quadratic final cost, we can relate it to the feedforward corrections $\sysInput_L$ using our linearized model \eqref{liftedLTV}
%
\begin{equation}
\begin{aligned}
\ValueFunction(\sysInput_L) &= \error_{N}^{\mathrm{T}}\vec{Q}_{N}\error_{N}, \\
&= (\vec{F}_N\sysInput_L + \linDist_N)^{\mathrm{T}}\vec{Q}_{N}(\vec{F}_N\sysInput_L + \linDist_N).
\end{aligned}
\label{finalCost}
\end{equation}
%
\noindent where $\vec{F}_N$ is the block row entry of $\vec{F}$ in \eqref{Fmatrix} corresponding to the hitting time $t = N$
%
\begin{equation*}
\begin{aligned}
 \vec{F}_N &= 
 \begin{bmatrix}
  \vec{A}_{N-1} \ldots \vec{A}_2 \vec{B}_1 & \cdots & \vec{B}_N 
 \end{bmatrix}.
\end{aligned}
\end{equation*}
%
\noindent Using Newton's method on \eqref{finalCost} we reach the reweighted form of \eqref{ILCPlantInversion}
%
\begin{equation}
\begin{aligned}
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}\error_k,\\
\end{aligned}
\label{ILCmayerForm}
\end{equation}
%
\noindent here $\vec{M}$ is defined as the matrix \eqref{Qmatrix} where all the diagonal entries $\vec{Q}_{t}$ are set to zero except for the last block entry $\vec{Q}_{N}$ corresponding to hitting time.


\subsection{Iterative Learning Control for Movement Primitives}\label{ilcOnDMP} 

Execution errors in tracking the rhythmic DMP prevents us from initializing the robot at each iteration to the same state. Starting from different initial conditions $\state_0 = [\joint^{\intercal}_0,\dot{\joint}^{\intercal}_0]^{\mathrm{T}}$ we can consider a movement primitive as in \eqref{dmp} to give a smooth and feasible interpolating trajectory that is similar to the demonstrations.
For such movement primitives we consider the Mayer form ILC update \eqref{ILCmayerForm} to be the natural candidate since in each trial the DMPs generate different trajectories. Trying to track these varying trajectories with an ILC update law as in \eqref{ILCNewtonsMethod} can make learning unstable.

To compensate for the varying initial conditions we recompute the reference control input $\trjInput$ based on the nominal inverse dynamics model. With this correction the total feedforward control commands $\sysInput_{\mathrm{\,f\,f}}$ at iteration $k+1$ are computed as follows
%
\begin{equation}
\begin{aligned}
\sysInput_{\mathrm{\,f\,f}} &= \trjInput + \sysInput_{k+1}, \\
\sysInput_{k+1} &= \sysInput_k - (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}\error_k.\\
\end{aligned}
\label{ILCfinalForm}
\end{equation}
%
\noindent Equation \eqref{ILCfinalForm} is referred to as the goal based ILC or $\alg$.


\subsection{Algorithm \& Implementation}\label{algorithm}

We use the update law derived in \eqref{ILCfinalForm} in our algorithm, given in Algorithm~\ref{alg1}. After the strike is completed, the rhythmic DMP can be further evolved to bring it close to $\state_0$. We consider only the errors on this striking part. This update law enables us to take advantage of the superlinear order of convergence property of Newton's method based descent methods while ensuring robustness on the variations in initial conditions $\vec{\delta}_k$. 

Depending on the task and models available, different weighting matrices $\vec{M}$ and $\vec{R}$ can be incorporated as desired. $\vec{M}$ can be designed to consider not just the hitting state, but a hitting segment of the rhythmic DMP. Iteration dependent $\vec{R}_k$ will ensure that the learning performance does not degrade with additional regularization, and one can reduce the weighting as the errors get smaller. The practitioner can additionally ensure safety by applying a learning rate $\beta$. For simplicity we only show the learning rate in Algorithm~\ref{alg1}.%We discuss the additional degrees of freedom of the algorithm in more detail in section \ref{experiments}.

%We initialize the algorithm $\alg$ with the necessary weighting and transition matrices. The transition matrix $A_{\fullvec}$ is derived by linearizing the nominal robot dynamics around the given reference trajectory $r$. Nominal inputs are acquired using the inverse dynamics model. Weights of the DMPs are initialized for each degree of freedom using regression on the reference trajectory.

% maybe reference needed
Notice that methods based on differential dynamic programming, such as iLQR, show a stronger dependency on model accuracy since they update both the reference trajectory $\traj(t)$ and the nominal inputs $\sysInput_{\mathrm{IDM}}$, often relying on re-computation in short horizons and on efficient simulators~\cite{Tassa08} to account for model inaccuracies. 

Under high mismatch cases where the input-to-output matrix $\vec{F}$ differs from the actual dynamics of the system substantially\footnote{Dynamics mismatch can be quantified with the maximum singular value of the difference.}, the robust convergence criteria~\cite{Bristow06} will not be satisfied and exploration in joint space will be necessary to improve the model and adapt $\vec{F}$. We leave the extension of ILC to such adaptive settings for future work.

\begin{algorithm}[tb]
   \caption{\alg}
   \label{alg1}
\begin{algorithmic}
   \STATE {\bfseries Input:} $\threshold > 0$, $\vec{Q}_N, \vec{M} \succeq 0$, $0 < \beta \leq 1$, $\vec{\dmp}$ 
   \STATE Form $\vec{F}$ using $\vec{A}$, $\vec{B}$ matrices
   \STATE Initialize $k = 1$, $\ilcInput = \vec{0}$
   \REPEAT 
   	   \STATE Rollout $\vec{\dmp}$ from $\state_0 + \vec{\delta}_k$ 
   	   \STATE Compute $\trjInput$ with inverse dynamics
 	   \STATE Strike with controls $\sysInput_{\mathrm{\,f\,f}} = \trjInput + \ilcInput$  %\sysInput = \nu - K_{\sysInput}(\state - \dmp(\weights)))$
 	   \STATE Observe $\error_k = \state_k - \vec{\dmp}$ from $\ddot{\joint} = \dynamics(\joint,\dot{\joint},\sysInput)$
 	   \STATE Compute $\ValueFunction_k$ = $\error_N^{\mathrm{T}}\vec{Q}_{N}\error_N$
 	   \STATE Update $\ilcInput \leftarrow \ilcInput - \beta (\vec{F}^{\mathrm{T}}\vec{M}\vec{F})^{-1}\vec{F}^{\mathrm{T}}\vec{M}\error_k$
 	   \STATE Follow $\vec{\dmp}$ to $\state_0$
   \UNTIL{$\ValueFunction_k < \threshold$}
\end{algorithmic}
\end{algorithm}